# -*- coding: utf-8 -*-
"""Capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2YuNFezaUL1nC9HFOZRW_C15cibqldR

#Capstone : NLP classification [Insincere Questions classification](https://github.com/yungnien/Springboard/blob/master/capstone/Capstone%20Proposal.ipynb)   

Sample qustions :

sincere (class 0) -->
How did Quebec nationalists see their province as a nation in the 1960s? 

insincere (class 1) -->
Which babies are more sweeter to their parents? Dark skin babies or light skin babies?
"""



"""#Summary 

This notebook contains the classification models for capstone project "Quora Insincere Questions Classification". This classification task is one of Kaggle competitions such that the data set is been used for this capstone project.
Both traditional machine learning models and deep learning models have been devel0ped and the results been compared

Traditional Machine Learning:
* Bag-Of-Word (BOW) Logistic Regression (TF-IDF)
* Word2Vec Logistic Regression
* Pre-Trained Word2Vec Logistic Regression  (GoogleNews-vectors-negative300)
* Doc2Vec Logistic Regression 

Deep Learning :
* RNN with Embedding Layer
* RNN with Pre_trained Embedding layer
* Language Models (Flair)
* Language Models (Universal Sentence Encoder)
* Language Models (ULMFiT)
"""



"""#0. Import Library, Define Functions and Constants"""

from google.colab import drive
drive.mount('/content/drive')

!pip install nltk
!pip install contractions
!pip install textsearch

import nltk
import re
import contractions
from nltk.tokenize import word_tokenize 
from nltk.corpus import  wordnet, stopwords 

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from joblib import dump, load

import gensim
from gensim.models import Word2Vec, KeyedVectors
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.test.utils import get_tmpfile

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, roc_curve, auc 
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import f1_score, precision_score, recall_score, zero_one_loss

import tensorflow as tf
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.models import Sequential, model_from_json
from tensorflow.python.keras.layers import Dense, GRU, Embedding, Dropout, Activation
from tensorflow.python.keras import metrics
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

def remove_white_space(text):
    return text.strip().strip('\t\n')

def remove_special_character(text):
    return re.sub('[^A-Za-z0-9\s]+', '', text)

def threeway_split(X, y):
    X_train, X_hold, y_train, y_hold  = train_test_split(X, y, 
                                                     train_size = 0.8, test_size = 0.2, 
                                                     random_state = 42, stratify = y)
    X_dev, X_test, y_dev, y_test  = train_test_split(X_hold, y_hold, 
                                                     train_size = 0.5, test_size = 0.5,  
                                                     random_state = 42, stratify = y_hold)

    print(len(X_train),len(X_dev), len(X_test))
    del X_hold, y_hold
    return X_train, X_dev, X_test, y_train, y_dev, y_test

def word_averaging(wv, words):
    all_words, mean = set(), []
    
    for word in words:
        if isinstance(word, np.ndarray):
            mean.append(word)
        elif word in wv.vocab:
            mean.append(wv.syn0norm[wv.vocab[word].index])
            all_words.add(wv.vocab[word].index)

    if not mean:
        #print("OOV, cannot compute similarity with no input %s", words)
        return np.zeros(wv.vector_size,)

    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)
    return mean

def  word_averaging_list(wv, docs):
    return np.vstack([word_averaging(wv, doc) for doc in docs])
  
def  word_doc2vec_list(model, docs):
    return np.vstack([model.infer_vector(doc.split()) for doc in docs])

def report(y, predicted):
    target_names = ['Sincere', 'Insincere']
        
    #classification_report 
    report = classification_report(y, predicted, target_names = target_names)
    print(report)
    
    #confusion matrix
    matrix = confusion_matrix(y, predicted)
    fig, ax = plt.subplots(figsize = (5,5))
    sns.heatmap(matrix, annot = True, fmt = 'd')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()
    return report, matrix
    
def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()
    
    
def plot_roc(y, predicted):
    #roc curve
    fpr, tpr, thresholds = roc_curve(y, predicted, pos_label = 1)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color ='blue', lw = 1, label = 'ROC curve for sincere (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color ='black', lw = 1, linestyle = '--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc = "lower right")
    plt.show()
    return roc_auc 
  
def store_matrix(name, y, pred):
    matrix_s = load(path +'matrix_project.sav')
    matrix_s[name] = {
        'Accuracy':accuracy_score(y, pred),
        'AUC':roc_auc_score(y, pred),
        'Precision (macro)':precision_score(y, pred, average='macro'),
        'Recall (macro)':recall_score(y, pred,average='macro'),
        'f1 (macro)':f1_score(y, pred, average='macro'),
        'misclassifications':zero_one_loss(y, pred)
    }
    dump(matrix_s, path+'matrix_project.sav')

## FUNCTIONS TAKEN FROM https://www.kaggle.com/gmhost/gru-capsule

def load_glove(word_index):
    EMBEDDING_FILE = path+"embeddings/glove.840B.300d/glove.840B.300d.txt"
    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]
    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE))
    
    all_embs = np.stack(embeddings_index.values())
    emb_mean,emb_std = -0.005838499,0.48782197
    embed_size = all_embs.shape[1]

    nb_words = min(num_words, len(word_index))
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
    for word, i in word_index.items():
        if i >= num_words: continue
        embedding_vector = embeddings_index.get(word)
        #ALLmight
        if embedding_vector is not None: 
            embedding_matrix[i] = embedding_vector
        else:
            embedding_vector = embeddings_index.get(word.capitalize())
            if embedding_vector is not None: 
                embedding_matrix[i] = embedding_vector
    return embedding_matrix 
    
            
def load_fasttext(word_index):    
    EMBEDDING_FILE = path+"embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec"
    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE) if len(o)>100)

    all_embs = np.stack(embeddings_index.values())
    emb_mean,emb_std = all_embs.mean(), all_embs.std()
    embed_size = all_embs.shape[1]

    nb_words = min(num_words, len(word_index))
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
    for word, i in word_index.items():
        if i >= num_words: continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None: embedding_matrix[i] = embedding_vector

    return embedding_matrix

def load_para(word_index):
    EMBEDDING_FILE = path+'embeddings/paragram_300_sl999/paragram_300_sl999.txt'
    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
    embeddings_index = dict(get_coefs(*o.split(" ")) for o in open(EMBEDDING_FILE, encoding="utf8", errors='ignore') if len(o)>100)

    all_embs = np.stack(embeddings_index.values())
    emb_mean,emb_std = -0.0053247833,0.49346462
    embed_size = all_embs.shape[1]

    nb_words = min(num_words, len(word_index))
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
    for word, i in word_index.items():
        if i >= num_words: continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None: embedding_matrix[i] = embedding_vector
    
    return embedding_matrix

# word limits
num_words = 8000 
# feature size
embedding_size = 300
# mean + 2 std
max_tokens = 30
pad = 'post'
path = '/content/drive/My Drive/data/'
matrix_s ={}

#dump(matrix_s, path+'matrix_project.sav')
#matrix_s = load(path +'matrix_project.sav')

"""#1. Simple Raw Text Clean Up 

To align the comparison of results, although the different model can take slightly different texts (such as cap word, punctuation symbols, etc), the data go through unified text cleaning processes, which are:

* NFKD normalization
* Contraction Fix
* White space removal 
* Case unification
* Special character removal 

Both stop words and numerical numbers been retained, also no stemming nor lemmatization. During the [data wrangling stage](https://github.com/yungnien/Springboard/blob/master/capstone/prototype/Data%20Wrangling.ipynb), I have reported the stop word removal, numerical number removal, stemming and lemmatization does not change the result significantly. As the deep learning model, especial the language model (LM), prefers the sentence is un-attacked, I decided to make a compromise to use the same data for all models.

<B>A balanced smaller data set also been sampled from the full data set for LM tuning. The full data set does not go through balancing attempt here as I already tested during prototyping using [imbalancing treatment](https://github.com/yungnien/Springboard/blob/master/capstone/prototype/Prototype-Imbalance.ipynb) which does not gain significant improvement.</B>
"""

train_data = pd.read_csv(path+'train.csv')
print('Training data set: ' + str(len(train_data)))

# simple text clean up
train_data['question_text'] = train_data['question_text'].str.normalize('NFKD')\
.apply(contractions.fix).apply(remove_white_space).str.lower().apply(remove_special_character)
train_data['word_count'] = train_data['question_text'].apply(lambda x: len(str(x).split()))

#remove empty text
train_data = train_data.loc[(train_data.word_count > 0)]
train_data= train_data.reset_index()

print('Training data set: ' + str(len(train_data)))
print(train_data.head())

dump(train_data, path+'train_data_project.sav')

train_data_s = pd.concat([train_data.loc[(train_data['target'] == 0) & (train_data['question_text'].str.len() > 10)].sample(n=90000, random_state=42),\
                          train_data.loc[(train_data['target'] == 1) & (train_data['question_text'].str.len() > 10)].sample(n=80000, random_state=42)], ignore_index=True)
train_data_s = train_data_s.sample(frac=1).reset_index(drop=True)
print('Training data set (small): ' + str(len(train_data_s)))
print(train_data_s.head())

dump(train_data, path+'train_data_s_project.sav')

train_text = train_data['question_text']
train_target = train_data['target']
dump(train_text, path+'train_text_project.sav')
dump(train_target, path+'target_project.sav')

train_text_s = train_data_s['question_text']
train_target_s = train_data_s['target']
dump(train_text_s, path+'train_text_s_project.sav')
dump(train_target_s, path+'target_s_project.sav')

"""#2. Data Split: 80% (train) , 10% (dev) ,  10 %(test)

As the data set has 1.3M records, which is relatively big and falls into "having large data" category. Instead of withholding large potion for development and test, I take 10% for each and keep 80% of data for training 

A tokenized data also prepared here for RNN model which padded to the length of 30. Again, during prototyping, the distribution for length of words in each question has been plotted that 30 words covered 95%+ of the cases and the pre pad and post pad make a minimum difference.
"""

X_train, X_dev, X_test, y_train, y_dev, y_test = threeway_split(train_text, train_target)
X_train_s, X_dev_s, X_test_s, y_train_s, y_dev_s, y_test_s = threeway_split(train_text_s, train_target_s)

## used in 7 & 8 (RNN model)
tokenizer = Tokenizer(num_words=num_words, lower=False, char_level=False)
tokenizer.fit_on_texts(train_data['question_text'])

X_train_token  = tokenizer.texts_to_sequences(X_train)
X_dev_token  = tokenizer.texts_to_sequences(X_dev)
X_test_token  = tokenizer.texts_to_sequences(X_test)

X_train_token = pad_sequences(X_train_token, maxlen=max_tokens, padding=pad, truncating=pad).tolist()
X_dev_token = pad_sequences(X_dev_token, maxlen=max_tokens, padding=pad, truncating=pad).tolist()
X_test_token = pad_sequences(X_test_token, maxlen=max_tokens, padding=pad, truncating=pad).tolist()

dump(tokenizer, path+'tokenizer_project.sav')
dump(X_train, path+'X_train_project.sav')
dump(X_train_token, path+'X_train_token_project.sav')
dump(y_train, path+'y_train_project.sav')
dump(X_dev, path+'X_dev_project.sav')
dump(X_dev_token, path+'X_dev_token_project.sav')
dump(y_dev, path+'y_dev_project.sav')
dump(X_test, path+'X_test_project.sav')
dump(X_test_token, path+'X_test_token_project.sav')
dump(y_test, path+'y_test_project.sav')

"""#3. BOW (TF-IDF)  Logistic Regression 

Several traditional [machine learning models](https://github.com/yungnien/Springboard/blob/master/capstone/prototype/Prototype-Sklearn.ipynb), including Naive Bayes, Logistic Regression, Support Vector Machine, RandomForest, and  Gradient Boosting, have been tested. The parameters fine-tuning plays some effect but does not dramatically change the accuracy order for the models. Among these models, the Logistic Regression holds a top position and computation time.  Therefore, the Logistic Regression model has been selected to carry out the following comparison. 

TF-IDF is a typical Bag-Of-Word vectoring approach used in NLP modeling, the word's frequency counting has a similar result that will not be included in this notebook. The word size (max feature)  was select mainly due to the restriction on available memory resource.
"""

logreg = Pipeline([('vect', CountVectorizer(max_features=num_words, min_df=2, lowercase=False)),
                   ('tfidf', TfidfTransformer()),
                   ('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),
                  ])
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_dev)
rep, mat = report(y_dev, y_pred)
roc_auc =plot_roc(y_dev, y_pred)
store_matrix("TF-IDF Logistic Regression (dev)", y_dev, y_pred)

y_pred = logreg.predict(X_train)
store_matrix("TF-IDF Logistic Regression (train)", y_train, y_pred)

del logreg

"""#4. Word2Vec Logistic Regression (Using Project Data)

[Word2Vec](https://arxiv.org/pdf/1301.3781.pdf), the word vector embedding is am important technic for NLP. The source of embedding can be from the project's data or existing pre-trained data. This section is using the project's data as the source for embedding. The selected dimension size is to align with the other word vector models to be compared with.  [Parameters](https://radimrehurek.com/gensim/models/word2vec.html) using default valuse except embedding_sez and min_count for reason of comparison. The default sg is 0(CBOW)     

As the word vector represents the dimensional vectors for each word, not the complete sentence. The completed dimensional vectors for the sentence needs to be aggregated in a consisted approach.  Therefore, the equal-weighted averaged vector is used for the word vector model, for both the pre-trained model and this one.
"""

word2Vec_model = Word2Vec(train_text.str.split(), size=embedding_size, min_count=2, workers=4)
word2Vec_model.save(path+"embeddings/word2vec.model")
#word2Vec_model = Word2Vec.load(path+"embeddings/word2vec.model")

word2Vec_model = Word2Vec.load(path+"embeddings/word2vec.model")
quorawv = word2Vec_model.wv
quorawv.init_sims(replace=True)
del word2Vec_model

X_train_word_average = word_averaging_list(quorawv,X_train.str.split())
X_dev_word_average = word_averaging_list(quorawv,X_dev.str.split())

embeddinglogreg = Pipeline([('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),
                  ])
embeddinglogreg.fit(X_train_word_average, y_train)

y_pred = embeddinglogreg.predict(X_dev_word_average)
report(y_dev, y_pred)
plot_roc(y_dev, y_pred)
store_matrix("Word2Vec Logistic Regression (dev)", y_dev, y_pred)

y_pred = embeddinglogreg.predict(X_train_word_average)
store_matrix("Word2Vec Logistic Regression (train)", y_train, y_pred)
del embeddinglogreg

"""#5. Pre-Trained word2Vec Logistic Regression

There are four pre-trained words vector models, including:

* glove.840B.300dm
* GoogleNews-vectors-negative300
* paragram_300_sl999
* wiki-news-300d-1

A uniformed approach, load the word vector matrix and retrieve the averaged dimensional vectors for each sentence has been applied. Follow the link can find the [ preliminary prototype on embedding/transfer](https://github.com/yungnien/Springboard/blob/master/capstone/prototype/Prototype_tansfer.ipynb)
"""

googlewv = KeyedVectors.load_word2vec_format(path+"embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin", binary=True)
googlewv.init_sims(replace=True)

X_train_word_average = word_averaging_list(googlewv,X_train.str.split())
X_dev_word_average = word_averaging_list(googlewv,X_dev.str.split())

embeddinglogreg = Pipeline([('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),
                  ])
embeddinglogreg.fit(X_train_word_average, y_train)

y_pred = embeddinglogreg.predict(X_dev_word_average)
report(y_dev, y_pred)
plot_roc(y_dev, y_pred)
store_matrix("Pre-Trained GoogleNew Logistic Regression (dev)", y_dev, y_pred)

y_pred = embeddinglogreg.predict(X_train_word_average)
store_matrix("Pre-Trained GoogleNew Logistic Regression (train)", y_train, y_pred)
del embeddinglogreg, googlewv

glove2word2vec(path+"embeddings/glove.840B.300d/glove.840B.300d.txt", path+"embeddings/glove.840B.300d/temp.txt")
glovewv = KeyedVectors.load_word2vec_format(path+"embeddings/glove.840B.300d/temp.txt", binary=False)
glovewv.init_sims(replace=True)

X_train_word_average = word_averaging_list(glovewv,X_train.str.split())
X_dev_word_average = word_averaging_list(glovewv,X_dev.str.split())

embeddinglogreg = Pipeline([('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),
                  ])
embeddinglogreg.fit(X_train_word_average, y_train)

y_pred = embeddinglogreg.predict(X_dev_word_average)
report(y_dev, y_pred)
plot_roc(y_dev, y_pred)
store_matrix("Pre-Trained Glove Logistic Regression (dev)", y_dev, y_pred)

y_pred = embeddinglogreg.predict(X_train_word_average)
store_matrix("Pre-Trained Glove Logistic Regression (train)", y_train, y_pred)
del embeddinglogreg, glovewv

glove2word2vec(path+"embeddings/paragram_300_sl999/paragram_300_sl999.txt", path+"embeddings/paragram_300_sl999/temp.txt")
paragramwv = KeyedVectors.load_word2vec_format(path+"embeddings/paragram_300_sl999/temp.txt", encoding='ISO-8859-1', unicode_errors='ignore', binary=False)
paragramwv.init_sims(replace=True)

X_train_word_average = word_averaging_list(paragramwv,X_train.str.split())
X_dev_word_average = word_averaging_list(paragramwv,X_dev.str.split())

embeddinglogreg = Pipeline([('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),
                  ])
embeddinglogreg.fit(X_train_word_average, y_train)

y_pred = embeddinglogreg.predict(X_dev_word_average)
report(y_dev, y_pred)
plot_roc(y_dev, y_pred)
store_matrix("Pre-Trained Paragram Logistic Regression (dev)", y_dev, y_pred)

y_pred = embeddinglogreg.predict(X_train_word_average)
store_matrix("Pre-Trained Paragram Logistic Regression (train)", y_train, y_pred)
del embeddinglogreg, paragramwv

wikiwv = KeyedVectors.load_word2vec_format(path+"embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec", binary=False)
wikiwv.init_sims(replace=True)

X_train_word_average = word_averaging_list(wikiwv,X_train.str.split())
X_dev_word_average = word_averaging_list(wikiwv,X_dev.str.split())

embeddinglogreg = Pipeline([('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),
                  ])

embeddinglogreg.fit(X_train_word_average, y_train)

y_pred = embeddinglogreg.predict(X_dev_word_average)
report(y_dev, y_pred)
plot_roc(y_dev, y_pred)
store_matrix("Pre-Trained Wiki Logistic Regression (dev)", y_dev, y_pred)

y_pred = embeddinglogreg.predict(X_train_word_average)
store_matrix("Pre-Trained Wiki Logistic Regression (train)", y_train, y_pred)
del embeddinglogreg, wikiwv



"""#6. Doc2Vec Logistic Regression (Project Data)

[Doc2Vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) is a variation/enhancement of Word2Vec model. Instead of learning the dimensional vector for each word, it includes a unique id for the training document to better represents the intended training.  As there are not many pre-trained dov2vec models, the project's data is used to create a model here.  Both the word size and dimension size been applied to maintain comparability (also not to crash the runtime session). there is the [parameters list](https://radimrehurek.com/gensim/models/doc2vec.html), the default dm = 1 (distributed memory)
"""

documents = [TaggedDocument(doc.split(), [i]) for i, doc in X_train.items()]

doc2vec_model = Doc2Vec(documents, max_vocab_size=num_words, dm=1,\
                        alpha=0.065, min_alpha=0.00025, negative=5,\
                        min_count=2, vector_size=embedding_size, window=5, workers=4)


fname = get_tmpfile(path+"doc2vec_model")
doc2vec_model.save(fname)
doc2vec_model = Doc2Vec.load(fname)

X_train_doc2vec = word_doc2vec_list(doc2vec_model,X_train)
X_dev_doc2vec = word_doc2vec_list(doc2vec_model,X_dev)
doclogreg = Pipeline([('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),
                  ])
doclogreg.fit(X_train_doc2vec, y_train)

y_pred = doclogreg.predict(X_dev_doc2vec)
report(y_dev, y_pred)
plot_roc(y_dev, y_pred)
store_matrix("Doc2Vec (PV-DM) Logistic Regression (dev)", y_dev, y_pred)


y_pred = doclogreg.predict(X_train_doc2vec)
store_matrix("Doc2Vec (PV-DM) Logistic Regression (train)", y_train, y_pred)
del doclogreg, doc2vec_model



doc2vec_model = Doc2Vec(documents, max_vocab_size=num_words, dm=0,\
                        alpha=0.065, min_alpha=0.00025, negative=5,\
                        min_count=2, vector_size=embedding_size, window=5, workers=4)
fname = get_tmpfile(path+"doc2vec_model")
doc2vec_model.save(fname)
doc2vec_model = Doc2Vec.load(fname)
X_train_doc2vec = word_doc2vec_list(doc2vec_model,X_train)
X_dev_doc2vec = word_doc2vec_list(doc2vec_model,X_dev)

doclogreg = Pipeline([('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),
                  ])
doclogreg.fit(X_train_doc2vec, y_train)

y_pred = doclogreg.predict(X_dev_doc2vec)
report(y_dev, y_pred)
plot_roc(y_dev, y_pred)
store_matrix("Doc2Vec (PV-DBOW) Logistic Regression (dev)", y_dev, y_pred)

y_pred = doclogreg.predict(X_train_doc2vec)
store_matrix("Doc2Vec (PV-DBOW) Logistic Regression (train)", y_train, y_pred)
del doclogreg, doc2vec_model



"""#7.  RNN With Embedding Layer 

Deep Learning / Artificial Neural Network represents a new era for NLP. The key for the deep learning including data input (the raw numerical data o embedded vector) and the architecture of the network. 

Again, during [ neural netwotk prototyping](https://github.com/yungnien/Springboard/blob/master/capstone/prototype/Prototype-keras.ipynb), CNN, DNN & RNN have been tested and all obtained good results at a similar level.  Not to overwhelm by the different architectural design, an RNN been applied for both project data embedding and pre-trained embedding.  

The number of layers and the number of nodes in each layer does not present special reason but for demonstration only.  no optimization has been performed for this type of model. The [Keras RNN doc](https://keras.io/layers/recurrent/) is here for reference
"""

X_train_token = load(path +'X_train_token_project.sav')
X_dev_token = load(path +'X_dev_token_project.sav')
y_train = load(path +'y_train_project.sav')
y_dev = load(path +'y_dev_project.sav')

model = Sequential()
optimizer = Adam(lr=1e-3)
model.add(Embedding(input_dim=num_words, output_dim=embedding_size, input_length=max_tokens, name='layer_embedding'))
model.add(GRU(units=32, return_sequences=True))
model.add(GRU(units=16, dropout=0.5, return_sequences=True))
model.add(GRU(units=8, return_sequences=True))
model.add(GRU(units=4))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['AUC', 'accuracy'])
model.summary()

history = model.fit(np.array(X_train_token), y_train, validation_data=(np.array(X_dev_token),y_dev), epochs=5, batch_size=500)

predicted = model.predict(np.array(X_dev_token))
predicted = predicted.T[0]
cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])
report(y_dev, cls_pred)
plot_roc(y_dev, predicted)
plot_history(history)
store_matrix("RNN (dev)", y_dev, cls_pred)


predicted = model.predict(np.array(X_train_token))
predicted = predicted.T[0]
cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])
store_matrix("RNN (train)", y_train, cls_pred)
del model



"""#8. RNN With  Pre-Trained Vector  

Similar to the word vector model presented earlier, the word vector embedding layer can switch among different existing pre-trained model. some pre-trained models bee applied here.


In all the models, the word embedding layer is not trainable.  To fine-tune the model, the embedding layer can be reset to trainable to adjust the vector according to the application data.  Ideally, it should achieve a better score as the word vector can adept the domain knowledge within the training data. The weight unfreeze was not performed due to a large number of comparable models already existing in this notebook.
"""

glove_embeddings = load_glove(tokenizer.word_index)    
model = Sequential()
optimizer = Adam(lr=1e-3)
model.add(Embedding(weights=[glove_embeddings], trainable=False, input_dim=num_words, output_dim=embedding_size, input_length=max_tokens))
model.add(GRU(units=32, return_sequences=True))
model.add(GRU(units=16, dropout=0.5, return_sequences=True))
model.add(GRU(units=8, return_sequences=True))
model.add(GRU(units=4))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['AUC', 'accuracy'])
model.summary()

history = model.fit(np.array(X_train_token), y_train, validation_data=(np.array(X_dev_token),y_dev), epochs=4, batch_size=500)

predicted = model.predict(np.array(X_dev_token))
predicted = predicted.T[0]
cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])
report(y_dev, cls_pred)
plot_roc(y_dev, predicted)
plot_history(history)
store_matrix("Glove RNN (dev)", y_dev, cls_pred)

predicted = model.predict(np.array(X_train_token))
predicted = predicted.T[0]
cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])
store_matrix("Glove RNN (train)", y_train, cls_pred)
del model, glove_embeddings



paragram_embeddings = load_para(tokenizer.word_index) 
model = Sequential()
optimizer = Adam(lr=1e-3)
model.add(Embedding(weights=[paragram_embeddings], trainable=False, input_dim=num_words, output_dim=embedding_size, input_length=max_tokens))
model.add(GRU(units=32, return_sequences=True))
model.add(GRU(units=16, dropout=0.5, return_sequences=True))
model.add(GRU(units=8, return_sequences=True))
model.add(GRU(units=4))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['AUC', 'accuracy'])
model.summary()

history = model.fit(np.array(X_train_token), y_train, validation_data=(np.array(X_dev_token),y_dev), epochs=4, batch_size=500)

predicted = model.predict(np.array(X_dev_token))
predicted = predicted.T[0]
cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])
report(y_dev, cls_pred)
plot_roc(y_dev, predicted)
plot_history(history)
store_matrix("Paragram RNN (dev)", y_dev, cls_pred)

predicted = model.predict(np.array(X_train_token))
predicted = predicted.T[0]
cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])
store_matrix("Paragram RNN (train)", y_train, cls_pred)
del model, paragram_embeddings



fasttext_embeddings = load_fasttext(tokenizer.word_index) 
model = Sequential()
optimizer = Adam(lr=1e-3)
model.add(Embedding(weights=[fasttext_embeddings], trainable=False, input_dim=num_words, output_dim=embedding_size, input_length=max_tokens))
model.add(GRU(units=32, return_sequences=True))
model.add(GRU(units=16, dropout=0.5, return_sequences=True))
model.add(GRU(units=8, return_sequences=True))
model.add(GRU(units=4))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['AUC', 'accuracy'])
model.summary()

history = model.fit(np.array(X_train_token), y_train, validation_data=(np.array(X_dev_token),y_dev), epochs=4, batch_size=500)

predicted = model.predict(np.array(X_dev_token))
predicted = predicted.T[0]
cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])
report(y_dev, cls_pred)
plot_roc(y_dev, predicted)
plot_history(history)
store_matrix("Fasttext RNN (dev)", y_dev, cls_pred)

predicted = model.predict(np.array(X_train_token))
predicted = predicted.T[0]
cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])
store_matrix("Fasttext RNN (train)", y_train, cls_pred)
del model, fasttext_embeddings



"""#9. Language Model (Flair)


All the Language Models(LM) below have been made available in 2018. Most of LMs were trained in the unsupervised and for multi-tasking neural network. [Flair](https://github.com/zalandoresearch/flair) is the first in this sequence.  

As the LMs are pre-trained, the input formats need to follow the specification. Also, the weights of the model can be fine-tuning for the project.  The idea for pretained model is to able to use small data set too achieve the similar result as large data set. Hence, the small set is been used here.
"""

# separated library installation as the new lib changed tensoflow's version 
# that interfered with tokenizer used in earlier steps
!pip install flair
!pip install tensorflow-hub
!pip install fastai

import tensorflow_hub as hub

from flair.data_fetcher import NLPTaskDataFetcher
from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer
from flair.data import Sentence
from pathlib import Path

from fastai.text import *

X = X_train_s.to_list() + X_test_s.to_list() + X_dev_s.to_list()
y = y_train_s.to_list() + y_test_s.to_list() + y_dev_s.to_list()

data = pd.DataFrame(list(zip(y, X)), columns =['label','text'])
data['label'] = '__label__' + data['label'].map({1: 'Insincere', 0: 'Sincere'})

data.iloc[0:int(len(data)*0.8)].to_csv(path+'flair/trainflair.csv', sep='\t', index = False, header = False)
data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv(path+'flair/testflair.csv', sep='\t', index = False, header = False)
data.iloc[int(len(data)*0.9):].to_csv(path+'flair/devflair.csv', sep='\t', index = False, header = False);

corpus = NLPTaskDataFetcher.load_classification_corpus(Path(path+'flair/'),\
                                                       test_file='testflair.csv',\
                                                       dev_file='devflair.csv',\
                                                       train_file='trainflair.csv')

word_embeddings = [WordEmbeddings('glove'),\
                   FlairEmbeddings('news-forward-fast'),\
                   FlairEmbeddings('news-backward-fast')]

document_embeddings = DocumentLSTMEmbeddings(word_embeddings,
                                             hidden_size=512,\
                                             reproject_words=True,\
                                             reproject_words_dimension=256)

classifier = TextClassifier(document_embeddings,\
                            label_dictionary=corpus.make_label_dictionary(),\
                            multi_label=False)

trainer = ModelTrainer(classifier, corpus)

trainer.train(path+'flair/', max_epochs=20)

classifier = TextClassifier.load(path+'flair/best-model.pt')

# convert X_dev, the regular accuracy report set, to sentence object and perform prediction
sentences =[]
for s in X_dev.to_list():
  sentences.append(Sentence(s))
classifier.predict(sentences)

# convert prediction result to the format compile with the defined function
cls_pred, prob, prob_adj = [], [], []
for s in sentences:
  if s.labels[0].value == 'Sincere':
    cls_pred.append(0)
  else:
    cls_pred.append(1)
  prob.append(s.labels[0].score)

for i in range(0, len(prob)):
  if cls_pred[i] == 0:
    prob_adj.append(1-prob[i])
  else:
    prob_adj.append(prob[i])

report(y_dev, cls_pred)
plot_roc(y_dev.values, np.asarray(prob_adj))
store_matrix("Flair (dev)", y_dev, cls_pred)
del classifier, trainer

log = pd.read_csv(path+'flair/loss.tsv', sep='\t')
print(log.info())
print()

plt.figure(1)
plt.subplot(121)
plt.plot(log['EPOCH'], log['TRAIN_LOSS'], 'r', log['EPOCH'], log['DEV_LOSS'], 'g',\
         log['EPOCH'], log['TEST_LOSS'], 'b')

plt.subplot(122)
plt.plot(log['EPOCH'], log['DEV_PRECISION'] ,'r', log['EPOCH'], log['DEV_RECALL'] ,'bo',\
         log['EPOCH'], log['DEV_F1'], 'ro', log['EPOCH'], log['TEST_PRECISION'], 'g',\
         log['EPOCH'], log['TEST_RECALL'], 'bo',log['EPOCH'], log['TEST_F1'], 'go')
plt.show()

"""#10. Language Model (Universal Sentence Encoder)

[Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf) demonstrated here took two pre-trained models and set the trainable flag to False and True that makes four combinations.
Tis google universal sentence encoder is available from [tensor hub](https://tfhub.dev/google/universal-sentence-encoder/1)
"""

TOTAL_STEPS = 4000
STEP_SIZE = 500

my_checkpointing_config = tf.estimator.RunConfig(
    keep_checkpoint_max = 2, # Retain the 2 most recent checkpoints.
)

# Training input on the whole training set with no limit on training epochs.
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    {'sentence': X_train_s.values}, y_train_s.values, 
    batch_size=256, num_epochs=None, shuffle=True)
# Prediction on the whole training set.
predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(
    {'sentence': X_train_s.values}, y_train_s.values, shuffle=False)
# Prediction on the whole validation set.
predict_val_input_fn = tf.estimator.inputs.numpy_input_fn(
    {'sentence': X_dev_s.values},  y_dev_s.values, shuffle=False)
# Prediction on the test set.
predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(
    {'sentence': X_dev.values}, y_dev.values, shuffle=False)

def train_and_evaluate_with_sentence_encoder(hub_module, train_module=False, path=''):
    embedding_feature = hub.text_embedding_column(
        key='sentence', module_spec=hub_module, trainable=train_module)
  
    print()
    print('Training with', hub_module)
    print('Trainable is:', train_module)
  
    dnn = tf.estimator.DNNClassifier(
        hidden_units=[512, 128],
        feature_columns=[embedding_feature],
        n_classes=2,
        activation_fn=tf.nn.relu,
        dropout=0.1,
        optimizer=tf.train.AdagradOptimizer(learning_rate=0.005),
        model_dir=path,
        config=my_checkpointing_config)

    for step in range(0, TOTAL_STEPS+1, STEP_SIZE):
        print('Training for step =', step)
        dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)
        print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))
        print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))
        print('\n')

    predictions_train = get_predictions(estimator=dnn, input_fn=predict_train_input_fn)
    predictions_dev = get_predictions(estimator=dnn, input_fn=predict_test_input_fn)
    del dnn
    return predictions_train, predictions_dev
    

def get_predictions(estimator, input_fn):
    return [x["class_ids"][0] for x in estimator.predict(input_fn=input_fn)]
  
tf.logging.set_verbosity(tf.logging.ERROR)

predictions_train, predictions_dev = train_and_evaluate_with_sentence_encoder(
    "https://tfhub.dev/google/nnlm-en-dim128/1", path=path+'storage/models/nnlm-en-dim128_f/')


report(y_dev.values, predictions_dev)
plot_roc(y_dev.values, predictions_dev)
store_matrix("nnlm-en-dim128 (dev)", y_dev.values, predictions_dev)
store_matrix("nnlm-en-dim128 (train)", y_train_s.values, predictions_train)

predictions_train, predictions_dev = train_and_evaluate_with_sentence_encoder(
    "https://tfhub.dev/google/nnlm-en-dim128/1", train_module=True, path=path+'storage/models/nnlm-en-dim128_t/')

report(y_dev.values, predictions_dev)
plot_roc(y_dev.values, predictions_dev)
store_matrix("nnlm-en-dim128-with-training (dev)", y_dev.values, predictions_dev)
store_matrix("nnlm-en-dim128-with-training (train)", y_train_s.values, predictions_train)

predictions_train, predictions_dev = train_and_evaluate_with_sentence_encoder(
    "https://tfhub.dev/google/universal-sentence-encoder/2", path=path+'storage/models/use-512_f/')

report(y_dev.values, predictions_dev)
plot_roc(y_dev.values, predictions_dev)
store_matrix("use-512 (dev)", y_dev.values, predictions_dev)
store_matrix("use-512 (train)", y_train_s.values, predictions_train)

predictions_test, predictions_dev = train_and_evaluate_with_sentence_encoder(
    "https://tfhub.dev/google/universal-sentence-encoder/2", train_module=True, path=path+'storage/models/use-512_t/')


report(y_dev.values, predictions_dev)
plot_roc(y_dev.values, predictions_dev)
store_matrix("use-512-with-training (dev)", y_dev.values, predictions_dev)
store_matrix("use-512-with-training (train)", y_train_s.values, predictions_train)



"""#11. Language Model (FastAI/ULMFiT)

[ULMFiT](https://arxiv.org/pdf/1801.06146.pdf) is the last model in this LM sequence. From [FastAI](http://nlp.fast.ai/), here use both forward and backward sentence taining sequences.
"""

#fast ai use minimum text preprocessing, should I reflect back to raw data?!
col_names = ['labels','text']
df_trn = pd.DataFrame({'text':X_train_s.values, 'labels':y_train_s.values}, columns=col_names)
df_val = pd.DataFrame({'text':X_dev_s.values, 'labels':y_dev_s.values}, columns=col_names)

# Language model data
data_lm = TextLMDataBunch.from_df(path+'fastai', train_df=df_trn, valid_df=df_val)
data_lm.save('tmp_lm') 
#data_lm = load_data(path+'fastai', 'tmp_lm')
#data_lm.vocab.itos[:20]

data_lm_f = load_data(path+'fastai', 'tmp_lm') #forward
#data_lm_f.show_batch()

# Language model, train and save
learner_f = language_model_learner(data_lm_f, AWD_LSTM, drop_mult=0.5)
learner_f = learner_f.to_fp16(clip=0.1)
learner_f.lr_find()
learner_f.recorder.plot()

learner_f.fit_one_cycle(1, 1e-2)
learner_f.recorder.plot_lr(show_moms=True)
learner_f.unfreeze()
learner_f.fit_one_cycle(1, 1e-3)
learner_f.save_encoder('f_enc')

# Classifier model data
data_clas_f = TextClasDataBunch.from_df(path+'fastai', train_df=df_trn, valid_df=df_val, vocab=data_lm.train_ds.vocab, bs=32)
classifier_f = text_classifier_learner(data_clas_f, AWD_LSTM, drop_mult=0.5)
classifier_f.load_encoder('f_enc')
classifier_f.lr_find()
classifier_f.recorder.plot()
classifier_f.fit_one_cycle(1, 1e-2)
classifier_f.recorder.plot_losses()
classifier_f.freeze_to(-2)
classifier_f.fit_one_cycle(1, slice(5e-3/2., 5e-3))
classifier_f.recorder.plot_losses()
classifier_f.unfreeze()
classifier_f.fit_one_cycle(1, slice(2e-3/100, 2e-3))
classifier_f.save('fwd_clas')

data_lm_b = load_data(path+'fastai', 'tmp_lm',backwards=True) #backward
#data_lm_b.show_batch()

learner_b = language_model_learner(data_lm_b, AWD_LSTM, drop_mult=0.5)
learner_b = learner_b.to_fp16(clip=0.1)
learner_b.lr_find()
learner_b.recorder.plot()

#learner.fit_one_cycle(1, 2e-2, moms=(0.8,0.7), wd=0.1)
learner_b.fit_one_cycle(1, 1e-2)
learner_b.recorder.plot_lr(show_moms=True)
learner_b.unfreeze()
learner_b.fit_one_cycle(1, 1e-3)
learner_b.save_encoder('b_enc')

# Classifier model data
data_clas_b = TextClasDataBunch.from_df(path+'fastai', train_df=df_trn, valid_df=df_val, vocab=data_lm.train_ds.vocab, bs=32, backwards=True)
classifier_b = text_classifier_learner(data_clas_b, AWD_LSTM, drop_mult=0.5) #pretrained=False
classifier_b.load_encoder('b_enc')
classifier_b.lr_find()
classifier_b.recorder.plot()
classifier_b.fit_one_cycle(1, 1e-2) 
classifier_b.recorder.plot_losses()
classifier_b.freeze_to(-2)
classifier_b.fit_one_cycle(1, slice(5e-3/2., 5e-3))
classifier_b.recorder.plot_losses()
classifier_b.unfreeze()
classifier_b.fit_one_cycle(1, slice(2e-3/100, 2e-3))
classifier_b.save('bwd_clas')

# this get prediction for validation set 
pred_fwd,lbl_fwd = classifier_f.get_preds() 
pred_bwd,lbl_bwd = classifier_b.get_preds()
final_pred = (pred_fwd+pred_bwd)/2
pred = np.argmax(final_pred, axis = 1)
report(lbl_bwd, pred)
plot_roc(lbl_bwd, pred)

pred_f, pred_b = [], []
print(len(X_dev.values))
for s in X_dev.values:
  pred_f.append(classifier_f.predict(s))
  pred_b.append(classifier_b.predict(s))
dump(pred_f, path+'pred_f.sav')
dump(pred_b, path+'pred_b.sav')

pred_f = load(path+'pred_f.sav')
pred_b = load(path+'pred_b.sav')

pred_f_1 = list(list(zip(*pred_f))[1])
pred_b_1 = list(list(zip(*pred_b))[1])

report(y_dev.values, pred_f_1)
plot_roc(y_dev.values, pred_f_1)
store_matrix("ULMFiT forward (dev)", y_dev.values, pred_f_1)

report(y_dev.values, pred_b_1)
plot_roc(y_dev.values, pred_b_1)
store_matrix("ULMFiT backward (dev)", y_dev.values, pred_b_1)

pred_f_2 = list(list(zip(*pred_f))[2])
pred_b_2 = list(list(zip(*pred_b))[2])
pred =[]
for i in range(len(pred_f)):
  pred.append(np.argmax(pred_f_2[i] + pred_b_2[i]))

report(y_dev.values, pred)
plot_roc(y_dev.values, pred)
store_matrix("ULMFiT (dev)", y_dev.values, pred)



"""#12. Matrix Comparison 

This section reports the summary for the matrix from the above models.  Just a reminder that the models are not fine-tuned to reach the best performance and I do not make an argument on which is the best model for this question. However, if the target score if f1-score, the RNN and LM is the clear winner as they are more complicated model such that able to hold more information within the structure. 

Using f1 as sorting criteria, the top five modes are all embeding models and the top tree are embedding using RNN. 

Follw the RNN models are the language models and the logistic regression models perform less impressed.
"""

matrix_s = load(path +'matrix_project.sav')
matrix_dev, matrix_train = {},{}
for key in matrix_s:
    if key.find('dev') != -1:
      matrix_dev[key] = matrix_s[key]
    else:
      matrix_train[key] = matrix_s[key]

dev_df = pd.DataFrame.from_dict(matrix_dev, orient="index")
train_df = pd.DataFrame.from_dict(matrix_train, orient="index")

dev_df.sort_values('f1 (macro)', ascending=False)

train_df.sort_values('f1 (macro)', ascending=False)

