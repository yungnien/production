{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReFactory.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IHeHm59wozda",
        "wYrCm3qGfNWo",
        "CdE5ffY4fRA5",
        "N3D3pAqNfYJr",
        "OMqziVdRfcCt",
        "NIOjpdPWgBbR"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHeHm59wozda",
        "colab_type": "text"
      },
      "source": [
        "# init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Iv0T2ORpoH",
        "colab_type": "code",
        "outputId": "40a77130-21fe-4d56-e2cf-ef41850fab4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4mwXWUN2TJu",
        "colab_type": "code",
        "outputId": "bce9ca29-eda5-4cfa-cfdb-80585996fb37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tensorflow-hub"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/2a/ba0a3812e2a1de2cc4ee0ded0bdb750a7cef1631c13c78a4fc4ab042adec/contractions-0.0.21-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.21\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.0MB/s \n",
            "\u001b[?25hCollecting Unidecode (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 47.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81693 sha256=ff5659794ce1614deb3766179af2e7567659e7f4ea9b8bd481bad835e3e6f4b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch\n",
            "Successfully installed Unidecode-1.1.1 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYrCm3qGfNWo",
        "colab_type": "text"
      },
      "source": [
        "# data.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8PkbLUOUNRD",
        "colab_type": "code",
        "outputId": "104e0c49-e7b5-41ab-9ea9-f724f67ecf1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#for data cleanup /load\n",
        "import re\n",
        "import contractions\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "from joblib import dump, load\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "num_words = 8000 \n",
        "max_tokens = 30\n",
        "pad = 'post'\n",
        "path = '/content/drive/My Drive/data/'\n",
        "\n",
        "\n",
        "def remove_white_space(text):\n",
        "    return text.strip().strip('\\t\\n')\n",
        "\n",
        "def remove_special_character(text):\n",
        "    return re.sub('[^A-Za-z0-9\\s]+', '', text)\n",
        "\n",
        "def data_clean(train_data, filename):\n",
        "  # simple text clean up\n",
        "  train_data['question_text'] = train_data['question_text']\\\n",
        "  .str.normalize('NFKD').apply(contractions.fix).apply(remove_white_space)\\\n",
        "  .str.lower().apply(remove_special_character)\n",
        "  train_data['word_count'] = train_data['question_text'].apply(lambda x: len(str(x).split()))\n",
        "  #remove empty text\n",
        "  train_data = train_data.loc[(train_data.word_count > 0)]\n",
        "  train_data= train_data.reset_index()\n",
        "  dump(train_data, path+filename)\n",
        "  return train_data\n",
        "\n",
        "def str_clean(question):\n",
        "  return remove_special_character(remove_white_space(contractions.fix(unicodedata.normalize('NFKD', question))).lower())\n",
        "\n",
        "def threeway_split(X, y):\n",
        "    X_train, X_hold, y_train, y_hold  = train_test_split(X, y, \n",
        "                                                     train_size = 0.8, test_size = 0.2, \n",
        "                                                     random_state = 42, stratify = y)\n",
        "    X_dev, X_test, y_dev, y_test  = train_test_split(X_hold, y_hold, \n",
        "                                                     train_size = 0.5, test_size = 0.5,  \n",
        "                                                     random_state = 42, stratify = y_hold)\n",
        "    del X_hold, y_hold\n",
        "    return X_train, X_dev, X_test, y_train, y_dev, y_test\n",
        "\n",
        "def generate(filename):\n",
        "  train_data = pd.read_csv(path+filename)\n",
        "  train_data = data_clean(train_data, 'train_ref.pkl')\n",
        "  train_data_s = pd.concat([train_data.loc[(train_data['target'] == 0) & (train_data['question_text'].str.len() > 10)].sample(n=90000, random_state=42),\\\n",
        "                            train_data.loc[(train_data['target'] == 1) & (train_data['question_text'].str.len() > 10)].sample(n=80000, random_state=42)], ignore_index=True)\n",
        "  train_data_s = train_data_s.sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  X_train, X_dev, X_test, y_train, y_dev, y_test = threeway_split(train_data['question_text'], train_data['target'])\n",
        "  X_train_s, X_dev_s, X_test_s, y_train_s, y_dev_s, y_test_s = threeway_split(train_data_s['question_text'], train_data_s['target'])\n",
        "  print('Training data set (regular): ' + str(len(train_data)))\n",
        "  print('Training data set (small): ' + str(len(train_data_s)))\n",
        "  print(train_data_s.head())\n",
        "  \n",
        "  #for model 1\n",
        "  dump(X_train, path+'X_train_ref.pkl')\n",
        "  dump(y_train, path+'y_train_ref.pkl')\n",
        "  dump(X_dev, path+'X_dev_ref.pkl')\n",
        "  dump(y_dev, path+'y_dev_ref.pkl')\n",
        "  dump(X_test, path+'X_test_ref.pkl')\n",
        "  dump(y_test, path+'y_test_ref.pkl')\n",
        "\n",
        "  #for model 2\n",
        "  tokenizer = Tokenizer(num_words=num_words, lower=False, char_level=False)\n",
        "  tokenizer.fit_on_texts(train_data['question_text'])\n",
        "  # need tokenizer and padding for predict\n",
        "  X_train_token  = tokenizer.texts_to_sequences(X_train)\n",
        "  X_dev_token  = tokenizer.texts_to_sequences(X_dev)\n",
        "  X_test_token  = tokenizer.texts_to_sequences(X_test)\n",
        "  X_train_token = pad_sequences(X_train_token, maxlen=max_tokens, padding=pad, truncating=pad).tolist()\n",
        "  X_dev_token = pad_sequences(X_dev_token, maxlen=max_tokens, padding=pad, truncating=pad).tolist()\n",
        "  X_test_token = pad_sequences(X_test_token, maxlen=max_tokens, padding=pad, truncating=pad).tolist()\n",
        "  dump(tokenizer, path+'tokenizer_ref.pkl')\n",
        "  dump(X_train_token, path+'X_train_token_ref.pkl')\n",
        "  dump(X_dev_token, path+'X_dev_token_ref.pkl')\n",
        "  dump(X_test_token, path+'X_test_token_ref.pkl')\n",
        "\n",
        "  #for model 3\n",
        "  dump(X_train_s, path+'X_train_s_ref.pkl')\n",
        "  dump(y_train_s, path+'y_train_s_ref.pkl')\n",
        "  dump(X_dev_s, path+'X_dev_s_ref.pkl')\n",
        "  dump(y_dev_s, path+'y_dev_s_ref.pkl')\n",
        "  dump(X_test_s, path+'X_test_s_ref.pkl')\n",
        "  dump(y_test_s, path+'y_test_s_ref.pkl')\n",
        "  print(\"generate complete\")\n",
        "\n",
        "def test():\n",
        "  if len(load(path+'X_train_ref.pkl')) != len(load(path+'y_train_ref.pkl')):\n",
        "    return False\n",
        "  if len(load(path+'X_dev_ref.pkl')) != len(load(path+'y_dev_ref.pkl')):\n",
        "    return False\n",
        "  if len(load(path+'X_test_ref.pkl')) != len(load(path+'y_test_ref.pkl')):\n",
        "    return False\n",
        "  if len(load(path+'X_train_token_ref.pkl')) != len(load(path+'y_train_ref.pkl')):\n",
        "    return False\n",
        "  if len(load(path+'X_dev_token_ref.pkl')) != len(load(path+'y_dev_ref.pkl')):\n",
        "    return False\n",
        "  if len(load(path+'X_test_token_ref.pkl')) != len(load(path+'y_test_ref.pkl')):\n",
        "    return False  \n",
        "  if len(load(path+'X_train_s_ref.pkl')) != len(load(path+'y_train_s_ref.pkl')):\n",
        "    return False\n",
        "  if len(load(path+'X_dev_s_ref.pkl')) != len(load(path+'y_dev_s_ref.pkl')):\n",
        "    return False\n",
        "  if len(load(path+'X_test_s_ref.pkl')) != len(load(path+'y_test_s_ref.pkl')):\n",
        "    return False  \n",
        "  if len(load(path+'X_train_ref.pkl')) < 1000000:\n",
        "    return False  \n",
        "  if len(load(path+'X_train_s_ref.pkl')) < 100000:\n",
        "    return False  \n",
        "  print(\"test complete\")\n",
        "  return True\n",
        "\n",
        "generate('train.csv')\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data set (regular): 1306116\n",
            "Training data set (small): 170000\n",
            "     index                   qid  ... target  word_count\n",
            "0  1145688  e07b0fb24d12b851f1c2  ...      1           9\n",
            "1   318173  3e5b754e8fec9128cb0e  ...      0           8\n",
            "2  1235644  f2269597fb05a3682dab  ...      0          13\n",
            "3   617819  78fd3a6e150598a4d63b  ...      0           8\n",
            "4  1221373  ef61c75299dab48f2b93  ...      0           7\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "generate complete\n",
            "test complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdE5ffY4fRA5",
        "colab_type": "text"
      },
      "source": [
        "# lg train & predict.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kv6sU-4GyEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from joblib import dump, load\n",
        "\n",
        "num_words = 8000 \n",
        "path = '/content/drive/My Drive/data/'\n",
        "\n",
        "def lg_train(X_train, y_train):\n",
        "  logreg = Pipeline([('vect', CountVectorizer(max_features=num_words, min_df=2, lowercase=False)),\n",
        "                   ('tfidf', TfidfTransformer()),\n",
        "                   ('clf', LogisticRegressionCV(class_weight='balanced', cv=5, scoring='roc_auc', max_iter=1000,n_jobs=-1)),\n",
        "                  ])\n",
        "  logreg.fit(X_train, y_train)\n",
        "  dump(logreg, path+'logreg_ref.pkl')\n",
        "  print('complete the training')\n",
        "  return logreg\n",
        "\n",
        "def lg_predict(X_predict):\n",
        "  logreg = load(path +'logreg_ref.pkl')\n",
        "  y_pred = logreg.predict(X_predict)\n",
        "  print('return prediction')\n",
        "  return y_pred\n",
        "  \n",
        "def lg_test(X_dev, y_dev):\n",
        "  logreg = load(path +'logreg_ref.pkl')\n",
        "  y_pred = logreg.predict(X_dev)\n",
        "  return f1_score(y_dev, y_pred, average='weighted') > 0.90\n",
        "\n",
        "\n",
        "lg_train(load(path +'X_train_ref.pkl'),load(path +'y_train_ref.pkl'))\n",
        "test = lg_test(load(path +'X_dev_ref.pkl'),load(path +'y_dev_ref.pkl'))\n",
        "predict = lg_predict(load(path +'X_test_ref.pkl'))\n",
        "print(test)\n",
        "print(np.unique(predict, return_counts=True))\n",
        "\n",
        "#print(lg_predict([str_clean(\"Why did ancient Roman toilets have large openings in the front\")]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3D3pAqNfYJr",
        "colab_type": "text"
      },
      "source": [
        "# rnn train & predict.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OscASnajAG-n",
        "colab_type": "code",
        "outputId": "cefdca89-cd60-4c0f-aa34-a9185e6fa160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.models import Sequential, save_model, load_model\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from sklearn.metrics import f1_score\n",
        "from joblib import dump, load\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "num_words = 8000 \n",
        "embedding_size = 300\n",
        "max_tokens = 30\n",
        "pad = 'post'\n",
        "path = '/content/drive/My Drive/data/'\n",
        "\n",
        "def load_para(word_index):\n",
        "    EMBEDDING_FILE = path+'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    nb_words = min(num_words, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix\n",
        "\n",
        "def rnn_train():\n",
        "    tokenizer= load(path+'tokenizer_ref.pkl')\n",
        "    X_train_token = load(path +'X_train_token_project.sav')\n",
        "    X_dev_token = load(path +'X_dev_token_project.sav')\n",
        "    y_train = load(path +'y_train_project.sav')\n",
        "    y_dev = load(path +'y_dev_project.sav')\n",
        "    paragram_embeddings = load_para(tokenizer.word_index)\n",
        "    \n",
        "    model = Sequential()\n",
        "    optimizer = Adam(lr=1e-3)\n",
        "    model.add(Embedding(weights=[paragram_embeddings], trainable=False, input_dim=num_words, output_dim=embedding_size, input_length=max_tokens))\n",
        "    model.add(GRU(units=32, return_sequences=True))\n",
        "    model.add(GRU(units=16, dropout=0.5, return_sequences=True))\n",
        "    model.add(GRU(units=8, return_sequences=True))\n",
        "    model.add(GRU(units=4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['AUC', 'accuracy'])\n",
        "    model.summary()\n",
        "    history = model.fit(np.array(X_train_token), y_train, validation_data=(np.array(X_dev_token),y_dev), epochs=4, batch_size=500)\n",
        "    save_model(model,path+'rnn_model.h5')\n",
        "    print('train complete')\n",
        "\n",
        "def rnn_predict(X_token):\n",
        "    model = load_model(path+'rnn_model.h5')\n",
        "    predicted = model.predict(np.array(X_token))\n",
        "    predicted = predicted.T[0]\n",
        "    cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])\n",
        "    print('return prediction')\n",
        "    return cls_pred\n",
        "\n",
        "def rnn_test(X_token, y_value):\n",
        "    model = load_model(path+'rnn_model.h5')\n",
        "    predicted = model.predict(np.array(X_token))\n",
        "    predicted = predicted.T[0]\n",
        "    cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in predicted])  \n",
        "    return f1_score(y_value, cls_pred, average='weighted') > 0.90\n",
        "\n",
        "rnn_train()\n",
        "test = rnn_test(load(path +'X_dev_token_ref.pkl'),load(path +'y_dev_ref.pkl'))\n",
        "predict = rnn_predict(load(path+'X_test_token_ref.pkl'))\n",
        "print(test)\n",
        "print(np.unique(predict, return_counts=True))\n",
        "\n",
        "tokenizer= load(path+'tokenizer_ref.pkl')\n",
        "X_token = tokenizer.texts_to_sequences([\"Why did ancient Roman toilets have large openings in the front\"])\n",
        "X_token = pad_sequences(X_token, maxlen=max_tokens, padding=pad, truncating=pad).tolist()\n",
        "print(rnn_predict(X_token))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "True\n",
            "return prediction\n",
            "[0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMqziVdRfcCt",
        "colab_type": "text"
      },
      "source": [
        "# lm train & predict.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wkjdRUKNJOJ",
        "colab_type": "code",
        "outputId": "2ad38304-e2eb-4f53-fd8b-551533c12aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from joblib import dump, load\n",
        "\n",
        "TOTAL_STEPS = 4000\n",
        "STEP_SIZE = 500\n",
        "path = '/content/drive/My Drive/data/'\n",
        "\n",
        "\n",
        "#def lm_train():\n",
        "#def lm_predict(X_token):\n",
        "#def lm_test(X_token, y_value):\n",
        "  \n",
        "X_train_s = load(path +'X_train_s_ref.pkl')\n",
        "X_test_s = load(path +'X_test_s_ref.pkl')\n",
        "X_dev_s = load(path +'X_dev_s_ref.pkl')\n",
        "y_train_s = load(path +'y_train_s_ref.pkl')\n",
        "y_test_s = load(path +'y_test_s_ref.pkl')\n",
        "y_dev_s = load(path +'y_dev_s_ref.pkl')\n",
        "\n",
        "X_dev = load(path +'X_dev_ref.pkl')\n",
        "y_dev = load(path +'y_dev_ref.pkl')\n",
        "\n",
        "\n",
        "# Retain the 2 most recent checkpoints.\n",
        "my_checkpointing_config = tf.estimator.RunConfig(\n",
        "    keep_checkpoint_max = 2, \n",
        ")\n",
        "# Training input on the whole training set with no limit on training epochs.\n",
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    {'sentence': X_train_s.values}, y_train_s.values, \n",
        "    batch_size=256, num_epochs=None, shuffle=True)\n",
        "# Prediction on the whole training set.\n",
        "predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    {'sentence': X_train_s.values}, y_train_s.values, shuffle=False)\n",
        "# Prediction on the whole validation set.\n",
        "predict_val_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    {'sentence': X_dev_s.values},  y_dev_s.values, shuffle=False)\n",
        "# Prediction on the test set.\n",
        "predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    {'sentence': X_dev.values}, y_dev.values, shuffle=False)\n",
        "\n",
        "\n",
        "def get_predictions(estimator, input_fn):\n",
        "    return [x[\"class_ids\"][0] for x in estimator.predict(input_fn=input_fn)]\n",
        "\n",
        "def train_and_evaluate_with_sentence_encoder(hub_module, train_module=False, path=''):\n",
        "    embedding_feature = hub.text_embedding_column(\n",
        "        key='sentence', module_spec=hub_module, trainable=train_module)\n",
        "  \n",
        "    print('Training with', hub_module)\n",
        "    print('Trainable is:', train_module)\n",
        "  \n",
        "    dnn = tf.estimator.DNNClassifier(\n",
        "        hidden_units=[512, 128],\n",
        "        feature_columns=[embedding_feature],\n",
        "        n_classes=2,\n",
        "        activation_fn=tf.nn.relu,\n",
        "        dropout=0.1,\n",
        "        optimizer=tf.train.AdagradOptimizer(learning_rate=0.005),\n",
        "        model_dir=path,\n",
        "        config=my_checkpointing_config)\n",
        "\n",
        "    for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
        "        print('Training for step =', step)\n",
        "        dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
        "        print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n",
        "        print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))\n",
        "        print('\\n')\n",
        "    \n",
        "    predictions_train = get_predictions(estimator=dnn, input_fn=predict_train_input_fn)\n",
        "    predictions_dev = get_predictions(estimator=dnn, input_fn=predict_test_input_fn)\n",
        "    return predictions_train, predictions_dev, dnn\n",
        "    \n",
        "  \n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "predictions_test, predictions_dev, dnn = train_and_evaluate_with_sentence_encoder(\n",
        "    \"https://tfhub.dev/google/universal-sentence-encoder/2\", train_module=True, path=path+'storage/models/refact/')\n",
        "\n",
        "#report(y_dev.values, predictions_dev)\n",
        "#plot_roc(y_dev.values, predictions_dev)\n",
        "#store_matrix(\"use-512-with-training (dev)\", y_dev.values, predictions_dev)\n",
        "#store_matrix(\"use-512-with-training (train)\", y_train_s.values, predictions_train)\n",
        "\n",
        "\n",
        "\n",
        "import numpy \n",
        "x = ['documents required at the time of interview in sbi']\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    {'sentence': numpy.array(x)},shuffle=False)\n",
        "print(get_predictions(estimator=dnn, input_fn=input_fn))\n",
        "x = ['white felame is the best human']\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    {'sentence': numpy.array(x)},shuffle=False)\n",
        "print(get_predictions(estimator=dnn, input_fn=input_fn))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with https://tfhub.dev/google/universal-sentence-encoder/2\n",
            "Trainable is: True\n",
            "Training for step = 0\n",
            "Eval Metrics (Train): {'accuracy': 0.954625, 'accuracy_baseline': 0.5294118, 'auc': 0.98519826, 'auc_precision_recall': 0.97951037, 'average_loss': 0.14811938, 'label/mean': 0.47058824, 'loss': 18.950361, 'precision': 0.94910145, 'prediction/mean': 0.4588833, 'recall': 0.95478123, 'global_step': 500}\n",
            "Eval Metrics (Validation): {'accuracy': 0.90982354, 'accuracy_baseline': 0.5294118, 'auc': 0.9648245, 'auc_precision_recall': 0.9545516, 'average_loss': 0.23610932, 'label/mean': 0.47058824, 'loss': 30.179386, 'precision': 0.903935, 'prediction/mean': 0.45778364, 'recall': 0.9045, 'global_step': 500}\n",
            "\n",
            "\n",
            "Training for step = 500\n",
            "Eval Metrics (Train): {'accuracy': 0.9828456, 'accuracy_baseline': 0.5294118, 'auc': 0.99576074, 'auc_precision_recall': 0.99394524, 'average_loss': 0.06350087, 'label/mean': 0.47058824, 'loss': 8.124288, 'precision': 0.97501194, 'prediction/mean': 0.4706808, 'recall': 0.98889065, 'global_step': 1000}\n",
            "Eval Metrics (Validation): {'accuracy': 0.9077059, 'accuracy_baseline': 0.5294118, 'auc': 0.9618448, 'auc_precision_recall': 0.9511105, 'average_loss': 0.27002442, 'label/mean': 0.47058824, 'loss': 34.5144, 'precision': 0.8936222, 'prediction/mean': 0.47168648, 'recall': 0.9125, 'global_step': 1000}\n",
            "\n",
            "\n",
            "Training for step = 1000\n",
            "Eval Metrics (Train): {'accuracy': 0.9949191, 'accuracy_baseline': 0.5294118, 'auc': 0.9990585, 'auc_precision_recall': 0.99885386, 'average_loss': 0.021386046, 'label/mean': 0.47058824, 'loss': 2.7361262, 'precision': 0.99271536, 'prediction/mean': 0.4693182, 'recall': 0.99651563, 'global_step': 1500}\n",
            "Eval Metrics (Validation): {'accuracy': 0.8982941, 'accuracy_baseline': 0.5294118, 'auc': 0.9491111, 'auc_precision_recall': 0.9410149, 'average_loss': 0.38323706, 'label/mean': 0.47058824, 'loss': 48.98519, 'precision': 0.8890681, 'prediction/mean': 0.470136, 'recall': 0.895625, 'global_step': 1500}\n",
            "\n",
            "\n",
            "Training for step = 1500\n",
            "Eval Metrics (Train): {'accuracy': 0.9983603, 'accuracy_baseline': 0.5294118, 'auc': 0.9996644, 'auc_precision_recall': 0.9996455, 'average_loss': 0.007400051, 'label/mean': 0.47058824, 'loss': 0.946761, 'precision': 0.9977057, 'prediction/mean': 0.47008994, 'recall': 0.9988125, 'global_step': 2000}\n",
            "Eval Metrics (Validation): {'accuracy': 0.90017647, 'accuracy_baseline': 0.5294118, 'auc': 0.9399695, 'auc_precision_recall': 0.9395453, 'average_loss': 0.48777997, 'label/mean': 0.47058824, 'loss': 62.347816, 'precision': 0.8955692, 'prediction/mean': 0.46720585, 'recall': 0.891875, 'global_step': 2000}\n",
            "\n",
            "\n",
            "Training for step = 2000\n",
            "Eval Metrics (Train): {'accuracy': 0.9995735, 'accuracy_baseline': 0.5294118, 'auc': 0.99992, 'auc_precision_recall': 0.99991375, 'average_loss': 0.0025713954, 'label/mean': 0.47058824, 'loss': 0.32898378, 'precision': 0.999422, 'prediction/mean': 0.46991566, 'recall': 0.9996719, 'global_step': 2500}\n",
            "Eval Metrics (Validation): {'accuracy': 0.899, 'accuracy_baseline': 0.5294118, 'auc': 0.93415856, 'auc_precision_recall': 0.93431455, 'average_loss': 0.5347874, 'label/mean': 0.47058824, 'loss': 68.356285, 'precision': 0.8917571, 'prediction/mean': 0.470564, 'recall': 0.893875, 'global_step': 2500}\n",
            "\n",
            "\n",
            "Training for step = 2500\n",
            "Eval Metrics (Train): {'accuracy': 0.9998309, 'accuracy_baseline': 0.5294118, 'auc': 0.9999773, 'auc_precision_recall': 0.9999787, 'average_loss': 0.0010352853, 'label/mean': 0.47058824, 'loss': 0.13245419, 'precision': 0.9998594, 'prediction/mean': 0.4701756, 'recall': 0.99978125, 'global_step': 3000}\n",
            "Eval Metrics (Validation): {'accuracy': 0.8988235, 'accuracy_baseline': 0.5294118, 'auc': 0.9301374, 'auc_precision_recall': 0.9334755, 'average_loss': 0.6063085, 'label/mean': 0.47058824, 'loss': 77.498085, 'precision': 0.8928929, 'prediction/mean': 0.4691831, 'recall': 0.892, 'global_step': 3000}\n",
            "\n",
            "\n",
            "Training for step = 3000\n",
            "Eval Metrics (Train): {'accuracy': 0.9999706, 'accuracy_baseline': 0.5294118, 'auc': 0.99998593, 'auc_precision_recall': 0.99998415, 'average_loss': 0.00033400534, 'label/mean': 0.47058824, 'loss': 0.042732574, 'precision': 0.99995315, 'prediction/mean': 0.47050095, 'recall': 0.9999844, 'global_step': 3500}\n",
            "Eval Metrics (Validation): {'accuracy': 0.9002353, 'accuracy_baseline': 0.5294118, 'auc': 0.92961144, 'auc_precision_recall': 0.93382907, 'average_loss': 0.6444116, 'label/mean': 0.47058824, 'loss': 82.36841, 'precision': 0.89048564, 'prediction/mean': 0.4738727, 'recall': 0.8985, 'global_step': 3500}\n",
            "\n",
            "\n",
            "Training for step = 3500\n",
            "Eval Metrics (Train): {'accuracy': 0.99999267, 'accuracy_baseline': 0.5294118, 'auc': 0.99999297, 'auc_precision_recall': 0.9999922, 'average_loss': 0.00014954299, 'label/mean': 0.47058824, 'loss': 0.019132499, 'precision': 0.9999844, 'prediction/mean': 0.47059843, 'recall': 1.0, 'global_step': 4000}\n",
            "Eval Metrics (Validation): {'accuracy': 0.90170586, 'accuracy_baseline': 0.5294118, 'auc': 0.92977804, 'auc_precision_recall': 0.93421865, 'average_loss': 0.68233836, 'label/mean': 0.47058824, 'loss': 87.21618, 'precision': 0.88521, 'prediction/mean': 0.4830694, 'recall': 0.909, 'global_step': 4000}\n",
            "\n",
            "\n",
            "Training for step = 4000\n",
            "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.5294118, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 5.4999513e-05, 'label/mean': 0.47058824, 'loss': 0.0070366263, 'precision': 1.0, 'prediction/mean': 0.4705798, 'recall': 1.0, 'global_step': 4500}\n",
            "Eval Metrics (Validation): {'accuracy': 0.90170586, 'accuracy_baseline': 0.5294118, 'auc': 0.92884445, 'auc_precision_recall': 0.9341535, 'average_loss': 0.7057405, 'label/mean': 0.47058824, 'loss': 90.207436, 'precision': 0.88871145, 'prediction/mean': 0.47885436, 'recall': 0.904375, 'global_step': 4500}\n",
            "\n",
            "\n",
            "[0]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYRa3ry0RfXS",
        "colab_type": "code",
        "outputId": "11031223-93f8-4a0a-d6b9-048581e353c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# save model \n",
        "feature_spec = {\n",
        "    'sentence': tf.io.FixedLenFeature([], tf.string)\n",
        "}\n",
        "serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n",
        "export_dir = dnn.export_savedmodel(path+'storage/models/export', serving_input_receiver_fn)\n",
        "export_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'/content/drive/My Drive/data/storage/models/export/1569699519'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgyabqgwD_DU",
        "colab_type": "code",
        "outputId": "1169140a-be2a-4398-d931-7439a6fea95f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from joblib import dump, load\n",
        "import pandas as pd\n",
        "\n",
        "export_dir = b'/content/drive/My Drive/data/storage/models/export/1569699519'\n",
        "# retrive model \n",
        "predict_fn = tf.contrib.predictor.from_saved_model(export_dir)\n",
        "\n",
        "inputs = pd.DataFrame({\n",
        "    'sentence': ['documents required at the time of interview in sbi','white felame is the best human'],\n",
        "})\n",
        "\n",
        "examples = []\n",
        "for index, row in inputs.iterrows():\n",
        "    feature = {}\n",
        "    for col, value in row.iteritems():\n",
        "        value = str.encode(value)\n",
        "        feature[col] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature=feature\n",
        "        )\n",
        "    )\n",
        "    examples.append(example.SerializeToString())\n",
        "\n",
        "predictions = predict_fn({'inputs': examples})\n",
        "for score in predictions['scores']:\n",
        "    if score[0] > score[1]:\n",
        "        print('sincere')\n",
        "    else :\n",
        "        print('insincere')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/data/storage/models/export/1569699519/variables/variables\n",
            "sincere\n",
            "insincere\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIOjpdPWgBbR",
        "colab_type": "text"
      },
      "source": [
        "# utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrxBwf4BSSBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import contractions\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import  wordnet, stopwords \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from joblib import dump, load\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc \n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, zero_one_loss\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.models import Sequential, model_from_json\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, Dropout, Activation\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def remove_white_space(text):\n",
        "    return text.strip().strip('\\t\\n')\n",
        "\n",
        "def remove_special_character(text):\n",
        "    return re.sub('[^A-Za-z0-9\\s]+', '', text)\n",
        "\n",
        "def threeway_split(X, y):\n",
        "    X_train, X_hold, y_train, y_hold  = train_test_split(X, y, \n",
        "                                                     train_size = 0.8, test_size = 0.2, \n",
        "                                                     random_state = 42, stratify = y)\n",
        "    X_dev, X_test, y_dev, y_test  = train_test_split(X_hold, y_hold, \n",
        "                                                     train_size = 0.5, test_size = 0.5,  \n",
        "                                                     random_state = 42, stratify = y_hold)\n",
        "\n",
        "    print(len(X_train),len(X_dev), len(X_test))\n",
        "    del X_hold, y_hold\n",
        "    return X_train, X_dev, X_test, y_train, y_dev, y_test\n",
        "\n",
        "def remove_white_space(text):\n",
        "    return text.strip().strip('\\t\\n')\n",
        "\n",
        "def remove_special_character(text):\n",
        "    return re.sub('[^A-Za-z0-9\\s]+', '', text)\n",
        "\n",
        "\n",
        "def report(y, predicted):\n",
        "    target_names = ['Sincere', 'Insincere']\n",
        "        \n",
        "    #classification_report \n",
        "    report = classification_report(y, predicted, target_names = target_names)\n",
        "    print(report)\n",
        "    \n",
        "    #confusion matrix\n",
        "    matrix = confusion_matrix(y, predicted)\n",
        "    fig, ax = plt.subplots(figsize = (5,5))\n",
        "    sns.heatmap(matrix, annot = True, fmt = 'd')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "    return report, matrix\n",
        "    \n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "def plot_roc(y, predicted):\n",
        "    #roc curve\n",
        "    fpr, tpr, thresholds = roc_curve(y, predicted, pos_label = 1)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color ='blue', lw = 1, label = 'ROC curve for sincere (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color ='black', lw = 1, linestyle = '--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc = \"lower right\")\n",
        "    plt.show()\n",
        "    return roc_auc \n",
        "  \n",
        "def store_matrix(name, y, pred):\n",
        "    matrix_s = load(path +'matrix_project.sav')\n",
        "    matrix_s[name] = {\n",
        "        'Accuracy':accuracy_score(y, pred),\n",
        "        'AUC':roc_auc_score(y, pred),\n",
        "        'Precision (macro)':precision_score(y, pred, average='macro'),\n",
        "        'Recall (macro)':recall_score(y, pred,average='macro'),\n",
        "        'f1 (macro)':f1_score(y, pred, average='macro'),\n",
        "        'misclassifications':zero_one_loss(y, pred)\n",
        "    }\n",
        "    dump(matrix_s, path+'matrix_project.sav')\n",
        "    \n",
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        #print(\"OOV, cannot compute similarity with no input %s\", words)\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, docs):\n",
        "    return np.vstack([word_averaging(wv, doc) for doc in docs])\n",
        "  \n",
        "def  word_doc2vec_list(model, docs):\n",
        "    return np.vstack([model.infer_vector(doc.split()) for doc in docs])\n",
        "    \n",
        "## FUNCTIONS TAKEN FROM https://www.kaggle.com/gmhost/gru-capsule\n",
        "\n",
        "def load_glove(word_index):\n",
        "    EMBEDDING_FILE = path+\"embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "    \n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = -0.005838499,0.48782197\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    nb_words = min(num_words, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        #ALLmight\n",
        "        if embedding_vector is not None: \n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            embedding_vector = embeddings_index.get(word.capitalize())\n",
        "            if embedding_vector is not None: \n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix \n",
        "    \n",
        "            \n",
        "def load_fasttext(word_index):    \n",
        "    EMBEDDING_FILE = path+\"embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\"\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    nb_words = min(num_words, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def load_para(word_index):\n",
        "    EMBEDDING_FILE = path+'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    nb_words = min(num_words, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix\n",
        "  \n",
        "# word limits\n",
        "num_words = 8000 \n",
        "# feature size\n",
        "embedding_size = 300\n",
        "# mean + 2 std\n",
        "max_tokens = 30\n",
        "pad = 'post'\n",
        "path = '/content/drive/My Drive/data/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf-LUoA8bMRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}